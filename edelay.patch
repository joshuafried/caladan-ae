diff --git a/inc/iokernel/control.h b/inc/iokernel/control.h
index c35707a..b618117 100644
--- a/inc/iokernel/control.h
+++ b/inc/iokernel/control.h
@@ -31,14 +31,17 @@ struct q_ptrs {
 	uint32_t		storage_tail;
 	uint64_t		instr;
 	uint64_t		tsc;
-	uint64_t		oldest_tsc;
 };
 
 struct congestion_info {
 	float			load;
 	uint64_t		standing_queue_us;
-	uint64_t		rq_oldest_tsc;
-	uint64_t		pkq_oldest_tsc;
+	uint64_t		rq_qlen;
+	uint64_t		rq_elapsed;
+	uint64_t		rq_dequeued;
+	uint64_t		pkq_qlen;
+	uint64_t		pkq_elapsed;
+	uint64_t		pkq_dequeued;
 };
 
 enum {
diff --git a/inc/runtime/runtime.h b/inc/runtime/runtime.h
index 21e25c6..af274d0 100644
--- a/inc/runtime/runtime.h
+++ b/inc/runtime/runtime.h
@@ -38,26 +38,28 @@ static inline uint64_t runtime_standing_queue_us(void)
  */
 static inline uint64_t runtime_queue_us(void)
 {
-	uint64_t now;
-	uint64_t rq_oldest_tsc, pkq_oldest_tsc;
-	uint64_t rq_delay, pkq_delay;
+	uint64_t rq_qlen, rq_elapsed, rq_dequeued;
+	uint64_t pkq_qlen, pkq_elapsed, pkq_dequeued;
+	uint64_t rq_delay = 0;
+	uint64_t pkq_delay = 0;
 
 	if ((unsigned int)atomic_read(&runningks) < maxks)
 		return 0;
 
-	now = rdtsc();
-	rq_oldest_tsc = ACCESS_ONCE(runtime_congestion->rq_oldest_tsc);
-	pkq_oldest_tsc = ACCESS_ONCE(runtime_congestion->pkq_oldest_tsc);
-	rq_delay = 0;
-	pkq_delay = 0;
+	rq_qlen = ACCESS_ONCE(runtime_congestion->rq_qlen);
+	rq_elapsed = ACCESS_ONCE(runtime_congestion->rq_elapsed);
+	rq_dequeued = ACCESS_ONCE(runtime_congestion->rq_dequeued);
 
-	if (now > rq_oldest_tsc)
-		rq_delay = now - rq_oldest_tsc;
+	pkq_qlen = ACCESS_ONCE(runtime_congestion->pkq_qlen);
+	pkq_elapsed = ACCESS_ONCE(runtime_congestion->pkq_elapsed);
+	pkq_dequeued = ACCESS_ONCE(runtime_congestion->pkq_dequeued);
 
-	if (now > pkq_oldest_tsc)
-		pkq_delay = now - pkq_oldest_tsc;
+	if (rq_dequeued > 0)
+		rq_delay = rq_qlen * rq_elapsed / rq_dequeued;
+	if (pkq_dequeued > 0)
+		pkq_delay = pkq_qlen * pkq_elapsed / pkq_dequeued;
 
-	return (rq_delay + pkq_delay) / cycles_per_us;
+	return (rq_delay + pkq_delay);
 }
 
 /**
diff --git a/iokernel/control.c b/iokernel/control.c
index 8f4bc32..cda8d9b 100644
--- a/iokernel/control.c
+++ b/iokernel/control.c
@@ -62,7 +62,6 @@ static int control_init_hwq(struct shm_region *r,
 {
 	if (hs->hwq_type == HWQ_INVALID) {
 		h->enabled = false;
-		h->busy_since = UINT64_MAX;
 		return 0;
 	}
 
@@ -85,7 +84,6 @@ static int control_init_hwq(struct shm_region *r,
 	if (h->parity_byte_offset > (1 << h->descriptor_log_size))
 		return -EINVAL;
 
-	h->busy_since = UINT64_MAX;
 	h->last_head = 0;
 	h->last_tail = 0;
 
@@ -193,7 +191,6 @@ static struct proc *control_create_proc(mem_key_t key, size_t len, pid_t pid,
 				sizeof(struct q_ptrs));
 		if (!th->q_ptrs)
 			goto fail;
-		th->q_ptrs->oldest_tsc = UINT64_MAX;
 
 		ret = control_init_hwq(&reg, &s->direct_rxq, &th->directpath_hwq);
 		if (ret)
diff --git a/iokernel/defs.h b/iokernel/defs.h
index aca857f..0c51ed9 100644
--- a/iokernel/defs.h
+++ b/iokernel/defs.h
@@ -63,8 +63,6 @@ struct hwq {
 
 	uint32_t		last_tail;
 	uint32_t		last_head;
-
-	uint64_t		busy_since;
 };
 
 struct timer {
@@ -84,7 +82,6 @@ struct thread {
 	uint32_t		last_rq_tail;
 	uint32_t		last_rxq_head;
 	uint32_t		last_rxq_tail;
-	uint64_t		rxq_busy_since;
 	unsigned int		core;
 	unsigned int		at_idx;
 	unsigned int		ts_idx;
diff --git a/iokernel/sched.c b/iokernel/sched.c
index be3a9d0..611f368 100644
--- a/iokernel/sched.c
+++ b/iokernel/sched.c
@@ -45,9 +45,6 @@ static struct core_state state[NCPU];
 /* policy-specific operations (TODO: should be made configurable) */
 const struct sched_ops *sched_ops;
 
-/* current hardware timestamp */
-static uint64_t cur_tsc;
-
 /**
  * sched_steer_flows - redirects flows to active kthreads
  * @p: the proc for which to reallocate flows
@@ -259,17 +256,6 @@ static bool hardware_queue_congested(struct proc *p, struct thread *th, struct h
 		h->last_head = cur_head;
 	}
 
-	/* check whether hwq is empty */
-	if (cur_head == cur_tail) {
-		h->busy_since = UINT64_MAX;
-		return false;
-	}
-
-	/* check whether there was any progress on draining hwq or new packet
-	 * has arrived */
-	if (cur_tail != last_tail || h->busy_since == UINT64_MAX)
-		h->busy_since = cur_tsc;
-
 	return (th->active || (h->queue_steering && p->active_thread_count)) ? wraps_lt(cur_tail, last_head) :
 				 cur_head != cur_tail;
 }
@@ -282,13 +268,14 @@ static void sched_detect_congestion(struct proc *p)
 	uint32_t cur_tail, cur_head, last_head, last_tail;
 	uint64_t now, timer_tsc;
 	int i;
+	uint64_t rq_qlen = 0;
+	uint64_t rq_dequeued = 0;
+	uint64_t pkq_qlen = 0;
+	uint64_t pkq_dequeued = 0;
 
 	bitmap_init(threads, NCPU, false);
 	bitmap_init(ios, NCPU, false);
 
-	uint64_t rq_oldest_tsc = UINT64_MAX;
-	uint64_t pkq_oldest_tsc = UINT64_MAX;
-
 	/* detect uthread runqueue congestion */
 	for (i = 0; i < p->thread_count; i++) {
 		th = &p->threads[i];
@@ -296,13 +283,14 @@ static void sched_detect_congestion(struct proc *p)
 		cur_tail = load_acquire(&th->q_ptrs->rq_tail);
 		last_head = th->last_rq_head;
 		cur_head = ACCESS_ONCE(th->q_ptrs->rq_head);
-		rq_oldest_tsc = MIN(rq_oldest_tsc, ACCESS_ONCE(th->q_ptrs->oldest_tsc));
 		th->last_rq_head = cur_head;
 		th->last_rq_tail = cur_tail;
 		if (th->active ? wraps_lt(cur_tail, last_head) :
 				 cur_head != cur_tail) {
 			bitmap_set(threads, i);
 		}
+		rq_qlen += (int32_t)(cur_head - cur_tail);
+		rq_dequeued += (int32_t)(cur_tail - last_tail);
 	}
 
 	/* detect RX queue congestion */
@@ -319,20 +307,19 @@ static void sched_detect_congestion(struct proc *p)
 			bitmap_set(ios, i);
 		}
 
-		/* update oldest tsc of rxq */
-		if (cur_head == cur_tail)
-			th->rxq_busy_since = UINT64_MAX;
-		else if (cur_tail != last_tail || th->rxq_busy_since == UINT64_MAX)
-			th->rxq_busy_since = cur_tsc;
+		/* hwq : for now only directpath */
+		last_tail = th->directpath_hwq.last_tail;
+		last_head = th->directpath_hwq.last_head;
+		cur_tail = ACCESS_ONCE(*th->directpath_hwq.consumer_idx);
+		cur_head = hwq_find_head(&th->directpath_hwq, cur_tail, last_head);
+		pkq_qlen += (int32_t)(cur_head - cur_tail);
+		pkq_dequeued += (int32_t)(cur_tail - last_tail);
 
 		if (hardware_queue_congested(p, th, &th->directpath_hwq, true))
 			bitmap_set(ios, i);
 
 		if (hardware_queue_congested(p, th, &th->storage_hwq, true))
 			bitmap_set(ios, i);
-
-		pkq_oldest_tsc = MIN(pkq_oldest_tsc, th->directpath_hwq.busy_since);
-		pkq_oldest_tsc = MIN(pkq_oldest_tsc, th->rxq_busy_since);
 	}
 
 	/* detect expired timers */
@@ -349,7 +336,8 @@ static void sched_detect_congestion(struct proc *p)
 	}
 
 	/* notify the scheduler policy of the current congestion */
-	sched_ops->notify_congested(p, threads, ios, rq_oldest_tsc, pkq_oldest_tsc);
+	sched_ops->notify_congested(p, threads, ios, rq_qlen, rq_dequeued,
+				    pkq_qlen, pkq_dequeued);
 }
 
 /*
@@ -417,8 +405,7 @@ void sched_poll(void)
 	 * slow pass --- runs every IOKERNEL_POLL_INTERVAL
 	 */
 
-	cur_tsc = rdtsc();
-	now = (cur_tsc - start_tsc) / cycles_per_us;
+	now = microtime();
 	if (now - last_time >= IOKERNEL_POLL_INTERVAL) {
 		int i;
 
diff --git a/iokernel/sched.h b/iokernel/sched.h
index 615c68f..4e4616b 100644
--- a/iokernel/sched.h
+++ b/iokernel/sched.h
@@ -41,8 +41,9 @@ struct sched_ops {
 	 * congested or uncongested, driving core allocation decisions.
 	 */
 	void (*notify_congested)(struct proc *p, bitmap_ptr_t threads,
-			         bitmap_ptr_t io, uint64_t rq_oldest_tsc,
-				 uint64_t pkq_oldest_tsc);
+			         bitmap_ptr_t io, uint64_t rq_qlen,
+				 uint64_t rq_dequeued, uint64_t pkq_qlen,
+				 uint64_t pkq_dequeued);
 
 	/**
 	 * notify_core_needed - notifies the scheduler that a core is needed
diff --git a/iokernel/simple.c b/iokernel/simple.c
index f16c876..d6e5740 100644
--- a/iokernel/simple.c
+++ b/iokernel/simple.c
@@ -29,8 +29,12 @@ struct simple_data {
 	/* congestion info */
 	float			load;
 	uint64_t		standing_queue_us;
-	uint64_t		rq_oldest_tsc;
-	uint64_t		pkq_oldest_tsc;
+	uint64_t		rq_qlen;
+	uint64_t		rq_elapsed;
+	uint64_t		rq_dequeued;
+	uint64_t		pkq_qlen;
+	uint64_t		pkq_elapsed;
+	uint64_t		pkq_dequeued;
 	bool			waking;
 };
 
@@ -235,8 +239,25 @@ static void simple_update_congestion_info(struct simple_data *sd)
 		sd->standing_queue_us = 0;
 	ACCESS_ONCE(info->standing_queue_us) = sd->standing_queue_us;
 
-	ACCESS_ONCE(info->rq_oldest_tsc) = sd->rq_oldest_tsc;
-	ACCESS_ONCE(info->pkq_oldest_tsc) = sd->pkq_oldest_tsc;
+	ACCESS_ONCE(info->rq_qlen) = sd->rq_qlen;
+	if (sd->rq_dequeued > 0) {
+		ACCESS_ONCE(info->rq_elapsed) = sd->rq_elapsed;
+		ACCESS_ONCE(info->rq_dequeued) = sd->rq_dequeued;
+		sd->rq_elapsed = IOKERNEL_POLL_INTERVAL;
+		sd->rq_dequeued = 0;
+	} else if (sd->rq_qlen > 0) {
+		sd->rq_elapsed += IOKERNEL_POLL_INTERVAL;
+	}
+
+	ACCESS_ONCE(info->pkq_qlen) = sd->pkq_qlen;
+	if (sd->pkq_dequeued > 0) {
+		ACCESS_ONCE(info->pkq_elapsed) = sd->pkq_elapsed;
+		ACCESS_ONCE(info->pkq_dequeued) = sd->pkq_dequeued;
+		sd->pkq_elapsed = IOKERNEL_POLL_INTERVAL;
+		sd->pkq_dequeued = 0;
+	} else if (sd->pkq_qlen > 0) {
+		sd->pkq_elapsed += IOKERNEL_POLL_INTERVAL;
+	}
 
 	/* update the CPU load */
 	/* TODO: handle using more than guaranteed cores */
@@ -246,21 +267,18 @@ static void simple_update_congestion_info(struct simple_data *sd)
 }
 
 static void simple_notify_congested(struct proc *p, bitmap_ptr_t threads,
-				    bitmap_ptr_t io, uint64_t rq_oldest_tsc,
-				    uint64_t pkq_oldest_tsc)
+				    bitmap_ptr_t io, uint64_t rq_qlen,
+				    uint64_t rq_dequeued, uint64_t pkq_qlen,
+				    uint64_t pkq_dequeued)
 {
 	struct simple_data *sd = (struct simple_data *)p->policy_data;
 	int ret;
 	uint64_t now;
 
-	if (sd->threads_active < sd->threads_max) {
-		now = rdtsc();
-		sd->rq_oldest_tsc = now;
-		sd->pkq_oldest_tsc = now;
-	} else {
-		sd->rq_oldest_tsc = rq_oldest_tsc;
-		sd->pkq_oldest_tsc = pkq_oldest_tsc;
-	}
+	sd->rq_qlen = rq_qlen;
+	sd->rq_dequeued += rq_dequeued;
+	sd->pkq_qlen = pkq_qlen;
+	sd->pkq_dequeued += pkq_dequeued;
 
 	/* do nothing if we woke up a core during the last interval */
 	if (sd->waking) {
diff --git a/runtime/defs.h b/runtime/defs.h
index a3371f7..edc7c87 100644
--- a/runtime/defs.h
+++ b/runtime/defs.h
@@ -104,7 +104,6 @@ struct thread {
 	unsigned int		state;
 	unsigned int		stack_busy;
 	unsigned int		last_cpu;
-	uint64_t		ready_tsc;
 };
 
 typedef void (*runtime_fn_t)(void);
diff --git a/runtime/sched.c b/runtime/sched.c
index e729b7b..f1456c0 100644
--- a/runtime/sched.c
+++ b/runtime/sched.c
@@ -155,25 +155,6 @@ static bool work_available(struct kthread *k)
 	        !list_empty(&k->rq_overflow) || softirq_work_available(k);
 }
 
-static void update_oldest_tsc(struct kthread *k)
-{
-	/* oldest thread in runqueue */
-	thread_t *th;
-	uint64_t oldest_tsc = UINT64_MAX;
-
-	assert_spin_lock_held(&k->lock);
-
-	if (load_acquire(&k->rq_head) - k->rq_tail)
-		th = k->rq[k->rq_tail % RUNTIME_RQ_SIZE];
-	else
-		th = list_top(&k->rq_overflow, struct thread, link);
-
-	if (th)
-		oldest_tsc = th->ready_tsc;
-
-	ACCESS_ONCE(k->q_ptrs->oldest_tsc) = oldest_tsc;
-}
-
 static bool steal_work(struct kthread *l, struct kthread *r)
 {
 	thread_t *th;
@@ -200,7 +181,6 @@ static bool steal_work(struct kthread *l, struct kthread *r)
 			list_add_tail(&l->rq_overflow, &r->rq[rq_tail++ % RUNTIME_RQ_SIZE]->link);
 		store_release(&r->rq_tail, rq_tail);
 		ACCESS_ONCE(r->q_ptrs->rq_tail) += avail;
-		update_oldest_tsc(r);
 		spin_unlock(&r->lock);
 
 		l->rq_head = lrq_head;
@@ -213,7 +193,6 @@ static bool steal_work(struct kthread *l, struct kthread *r)
 	th = list_pop(&r->rq_overflow, thread_t, link);
 	if (th) {
 		ACCESS_ONCE(r->q_ptrs->rq_tail)++;
-		update_oldest_tsc(r);
 		goto done;
 	}
 
@@ -374,8 +353,6 @@ done:
 	if (unlikely(!list_empty(&l->rq_overflow)))
 		drain_overflow(l);
 
-	update_oldest_tsc(l);
-
 	spin_unlock(&l->lock);
 
 	/* update exit stat counters */
@@ -422,7 +399,6 @@ static __always_inline void enter_schedule(thread_t *myth)
 	/* pop the next runnable thread from the queue */
 	th = k->rq[k->rq_tail++ % RUNTIME_RQ_SIZE];
 	ACCESS_ONCE(k->q_ptrs->rq_tail)++;
-	update_oldest_tsc(k);
 	spin_unlock(&k->lock);
 
 	/* increment the RCU generation number (odd is in thread) */
@@ -525,7 +501,6 @@ void thread_ready(thread_t *th)
 		STAT(REMOTE_WAKES)++;
 	rq_tail = load_acquire(&k->rq_tail);
 	ACCESS_ONCE(k->q_ptrs->rq_head)++;
-	th->ready_tsc = rdtsc();
 	if (unlikely(k->rq_head - rq_tail >= RUNTIME_RQ_SIZE)) {
 		assert(k->rq_head - rq_tail == RUNTIME_RQ_SIZE);
 		spin_lock(&k->lock);
